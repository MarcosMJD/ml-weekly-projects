


- home page recommendations,  personalized to a user based on their known interests
- related item recommendations, are recommendations similar to a particular item.

- Items (also known as documents). The entities a system recommends.
- Query (also known as context). The information a system uses to make recommendations.
  - User information (id, items interacted with... demographics, purchase history, user's activity (views, likes, clicks, products added to shopping cart, time spent on the page, etc.))
    - Explicit feedback: likes ratings comments reviews
    - Implicit feedback: click on an item, view of a video, easy to collect, difficult to interprete (translate user behavior into user tastes. There are tools)
  - Context (time, user's device...)
- Embeddings: A mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space

- Architecture
  
<img width="500" src="https://developers.google.com/machine-learning/recommendation/images/Process.svg">

Candidate Generation Overview

|Type|Definition|Example| 
|---|---|---|
|content-based filtering|	Uses similarity between items to recommend items similar to what the user likes.	|If user A watches two cute cat videos, then the system can recommend cute animal videos to that user. |
|collaborative filtering |	Uses similarities between queries and items simultaneously to provide recommendations.|	If user A is similar to user B, and user B likes video 1, then the system can recommend video 1 to user A (even if user A hasnâ€™t seen any videos similar to video 1).|

<img width="500" src="https://developers.google.com/machine-learning/recommendation/images/Similarity.svg">


To determine the degree of similarity, most recommendation systems rely on one or more of the following:
- cosine
- dot product
- Euclidean distance

Items that appear very frequently in the training set (for example, popular YouTube videos) tend to have embeddings with large norms. If capturing popularity information is desirable, then you should prefer dot product.
Items that appear very rarely may not be updated frequently during training. Consequently, if they are initialized with a large norm, the system may recommend rare items over more relevant items. To avoid this problem, be careful about embedding initialization, and use appropriate regularization.


**Content-based Filtering Advantages & Disadvantages**

Advantages

- The model doesn't need any data about other users, since the recommendations are specific to this user. This makes it easier to scale to a large number of users.
- The model can capture the specific interests of a user, and can recommend niche items that very few other users are interested in.

Disadvantages

- Since the feature representation of the items are hand-engineered to some extent, this technique requires a lot of domain knowledge. Therefore, the model can only be as good as the hand-engineered features.
- The model can only make recommendations based on existing interests of the user. In other words, the model has limited ability to expand on the users' existing interests.

**Collaborative Filtering**

In practice, the embeddings can be learned automatically, which is the power of collaborative filtering models. In the next two sections, we will discuss different models to learn these embeddings, and how to train them.


Note: Matrix factorization typically gives a more compact representation than learning the full matrix. The full matrix has  entries, while the embedding matrices  have  entries, where the embedding dimension  is typically much smaller than  and . As a result, matrix factorization finds latent structure in the data, assuming that observations lie close to a low-dimensional subspace. In the preceding example, the values of n, m, and d are so low that the advantage is negligible. In real-world recommendation systems, however, matrix factorization can be significantly more compact than learning the full matrix.

**Matrix Factorization and objective Function**
https://developers.google.com/machine-learning/recommendation/collaborative/matrix?hl=en

**Weighted Matrix Factorization** decomposes the objective into the following two sums:

A sum over observed entries.
A sum over unobserved entries (treated as zeroes).

In practical applications, you also need to weight the observed pairs carefully. For example, frequent items (for example, extremely popular YouTube videos) or frequent queries (for example, heavy users) may dominate the objective function. You can correct for this effect by weighting training examples to account for item frequency. In other words, you can replace the objective function by:
where  is a function of the frequency of query i and item j.

Minimizing the Objective Function
Common algorithms to minimize the objective function include:

- Stochastic gradient descent (SGD) is a generic method to minimize loss functions.
- Weighted Alternating Least Squares (WALS) is specialized to this particular objective.

**Collaborative Filtering Advantages & Disadvantages**

Advantages
- No domain knowledge necessary
- Serendipity. The model can help users discover new interests
- Great starting point. From the feedback matrix, the matrix factorization model may be trained. In particular, the system doesn't need contextual features.  In practice, this can be used as one of multiple candidate generators.

- Cannot handle fresh items. cold-start problem. Fix: Heuristics to generate embeddings of fresh items. If the system does not have interactions, the system can approximate its embedding by averaging the embeddings of items from the same category, from the same uploader (in YouTube), and so on... Or in WALS, compute the embedding for the new item or new user. No full retrain.
- Hard to include side features for query/item




Filter bubble problem (too much filter and accuracy limits the experience) is fixed 
- by balancing accuracy with novelty (new pieces in the list of most popular recommended item) and diversity (widening the range of choices). Eg. in addition to what the algorithm outputas, add show the most popular content from other categories or a niche content.
- with serendipity. a recommended item shall be relevant, novel and unexpected.
  
Trust issue... explain the user why it sees what is recommended. Netflix: "Beacause you watched a feature (movie)). Amazon (gift ideas based on my shopping history).

Adaptation to change. Retrain the model on new data.